//! é›†æˆæµ‹è¯•å‘½ä»¤è¡Œå·¥å…·
//! 
//! æä¾›å‘½ä»¤è¡Œæ¥å£æ¥æ‰§è¡Œå„ç§ç±»å‹çš„é›†æˆæµ‹è¯•

use ai_contract_generator::{
    config::Config,
    test_runner::{IntegrationTestRunner, TestRunnerConfig},
    error::Result,
};
use clap::{Parser, Subcommand};
use tracing::{info, error, Level};
use tracing_subscriber;
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "integration-test")]
#[command(about = "AI Multi-Agent Contract Generator Integration Test Runner")]
#[command(version = "1.0.0")]
struct Cli {
    #[command(subcommand)]
    command: Commands,
    
    /// é…ç½®æ–‡ä»¶è·¯å¾„
    #[arg(short, long, value_name = "FILE")]
    config: Option<PathBuf>,
    
    /// æ—¥å¿—çº§åˆ«
    #[arg(short, long, default_value = "info")]
    log_level: String,
    
    /// æµ‹è¯•æŠ¥å‘Šè¾“å‡ºç›®å½•
    #[arg(short, long, default_value = "test_reports")]
    output_dir: String,
}

#[derive(Subcommand)]
enum Commands {
    /// è¿è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•å¥—ä»¶
    Full {
        /// æµ‹è¯•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        #[arg(short, long, default_value = "1800")]
        timeout: u64,
        
        /// æ˜¯å¦ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        #[arg(short, long)]
        detailed_report: bool,
        
        /// æœ€å¤§å¹¶å‘æµ‹è¯•æ•°
        #[arg(short, long, default_value = "4")]
        max_concurrent: usize,
    },
    
    /// è¿è¡Œç«¯åˆ°ç«¯åŠŸèƒ½æµ‹è¯•
    E2e {
        /// æµ‹è¯•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        #[arg(short, long, default_value = "900")]
        timeout: u64,
    },
    
    /// è¿è¡Œéƒ¨ç½²éªŒè¯æµ‹è¯•
    Deployment {
        /// ç¯å¢ƒç±»å‹ (dev, staging, prod)
        #[arg(short, long, default_value = "dev")]
        environment: String,
    },
    
    /// è¿è¡Œå®‰å…¨æ€§æµ‹è¯•
    Security {
        /// æ˜¯å¦åŒ…å«æ¸—é€æµ‹è¯•
        #[arg(short, long)]
        penetration_test: bool,
    },
    
    /// è¿è¡Œå¥åº·æ£€æŸ¥
    Health,
    
    /// è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    Benchmark {
        /// åŸºå‡†æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        #[arg(short, long, default_value = "300")]
        duration: u64,
        
        /// å¹¶å‘ç”¨æˆ·æ•°
        #[arg(short, long, default_value = "10")]
        concurrent_users: usize,
    },
    
    /// ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    Report {
        /// æŠ¥å‘Šç±»å‹ (html, json, pdf)
        #[arg(short, long, default_value = "html")]
        format: String,
        
        /// è¾“å…¥çš„æµ‹è¯•ç»“æœæ–‡ä»¶
        #[arg(short, long)]
        input: PathBuf,
    },
}

#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();
    
    // åˆå§‹åŒ–æ—¥å¿—
    init_logging(&cli.log_level)?;
    
    info!("å¯åŠ¨ AI Multi-Agent Contract Generator é›†æˆæµ‹è¯•å·¥å…·");
    
    // åŠ è½½é…ç½®
    let config = load_config(cli.config.as_deref()).await?;
    
    // æ‰§è¡Œå‘½ä»¤
    match cli.command {
        Commands::Full { timeout, detailed_report, max_concurrent } => {
            run_full_tests(config, &cli.output_dir, timeout, detailed_report, max_concurrent).await?;
        }
        Commands::E2e { timeout } => {
            run_e2e_tests(config, &cli.output_dir, timeout).await?;
        }
        Commands::Deployment { environment } => {
            run_deployment_tests(config, &cli.output_dir, &environment).await?;
        }
        Commands::Security { penetration_test } => {
            run_security_tests(config, &cli.output_dir, penetration_test).await?;
        }
        Commands::Health => {
            run_health_check(config).await?;
        }
        Commands::Benchmark { duration, concurrent_users } => {
            run_benchmark_tests(config, &cli.output_dir, duration, concurrent_users).await?;
        }
        Commands::Report { format, input } => {
            generate_report(&format, &input, &cli.output_dir).await?;
        }
    }
    
    info!("é›†æˆæµ‹è¯•å·¥å…·æ‰§è¡Œå®Œæˆ");
    Ok(())
}

/// åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
fn init_logging(log_level: &str) -> Result<()> {
    let level = match log_level.to_lowercase().as_str() {
        "trace" => Level::TRACE,
        "debug" => Level::DEBUG,
        "info" => Level::INFO,
        "warn" => Level::WARN,
        "error" => Level::ERROR,
        _ => Level::INFO,
    };
    
    tracing_subscriber::fmt()
        .with_max_level(level)
        .with_target(false)
        .with_thread_ids(true)
        .with_file(true)
        .with_line_number(true)
        .init();
    
    Ok(())
}

/// åŠ è½½é…ç½®æ–‡ä»¶
async fn load_config(config_path: Option<&std::path::Path>) -> Result<Config> {
    match config_path {
        Some(path) => {
            info!("ä»æ–‡ä»¶åŠ è½½é…ç½®: {:?}", path);
            Config::from_file(path).await
        }
        None => {
            info!("ä½¿ç”¨é»˜è®¤é…ç½®");
            Ok(Config::default())
        }
    }
}

/// è¿è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•å¥—ä»¶
async fn run_full_tests(
    config: Config,
    output_dir: &str,
    timeout: u64,
    detailed_report: bool,
    max_concurrent: usize,
) -> Result<()> {
    info!("å¼€å§‹æ‰§è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•å¥—ä»¶");
    
    let test_config = TestRunnerConfig {
        run_e2e_tests: true,
        run_deployment_validation: true,
        run_security_tests: true,
        test_timeout_seconds: timeout,
        report_output_path: output_dir.to_string(),
        generate_detailed_report: detailed_report,
        max_concurrent_tests: max_concurrent,
    };
    
    let runner = IntegrationTestRunner::new(test_config, config).await?;
    let results = runner.run_comprehensive_tests().await?;
    
    // æ‰“å°ç»“æœæ‘˜è¦
    println!("\nğŸ¯ é›†æˆæµ‹è¯•æ‰§è¡Œå®Œæˆ");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“Š æ‰§è¡Œæ‘˜è¦:");
    println!("   â€¢ æ€»æµ‹è¯•æ•°: {}", results.execution_summary.total_tests);
    println!("   â€¢ é€šè¿‡æµ‹è¯•: {} âœ…", results.execution_summary.passed_tests);
    println!("   â€¢ å¤±è´¥æµ‹è¯•: {} âŒ", results.execution_summary.failed_tests);
    println!("   â€¢ æˆåŠŸç‡: {:.2}% ğŸ“ˆ", results.execution_summary.success_rate);
    println!("   â€¢ æ‰§è¡Œæ—¶é•¿: {:?} â±ï¸", results.execution_summary.total_duration);
    println!("   â€¢ æ•´ä½“çŠ¶æ€: {:?} ğŸ¯", results.execution_summary.overall_status);
    
    if !results.recommendations.is_empty() {
        println!("\nğŸ’¡ æ”¹è¿›å»ºè®®:");
        for (i, recommendation) in results.recommendations.iter().enumerate() {
            println!("   {}. {}", i + 1, recommendation);
        }
    }
    
    println!("\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆåˆ°: {}", output_dir);
    
    Ok(())
}

/// è¿è¡Œç«¯åˆ°ç«¯åŠŸèƒ½æµ‹è¯•
async fn run_e2e_tests(config: Config, output_dir: &str, timeout: u64) -> Result<()> {
    info!("å¼€å§‹æ‰§è¡Œç«¯åˆ°ç«¯åŠŸèƒ½æµ‹è¯•");
    
    let test_config = TestRunnerConfig {
        run_e2e_tests: true,
        run_deployment_validation: false,
        run_security_tests: false,
        test_timeout_seconds: timeout,
        report_output_path: output_dir.to_string(),
        generate_detailed_report: true,
        max_concurrent_tests: 4,
    };
    
    let runner = IntegrationTestRunner::new(test_config, config).await?;
    let results = runner.run_comprehensive_tests().await?;
    
    if let Some(e2e_results) = results.e2e_results {
        println!("\nğŸ”„ ç«¯åˆ°ç«¯æµ‹è¯•ç»“æœ:");
        println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        println!("   â€¢ æ€»æµ‹è¯•æ•°: {}", e2e_results.tests.len());
        println!("   â€¢ æˆåŠŸæµ‹è¯•: {} âœ…", e2e_results.success_count());
        println!("   â€¢ å¤±è´¥æµ‹è¯•: {} âŒ", e2e_results.failure_count());
        
        // æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•
        for test in &e2e_results.tests {
            if !test.success {
                println!("   âŒ {}: {}", test.name, 
                    test.error_message.as_ref().unwrap_or(&"æœªçŸ¥é”™è¯¯".to_string()));
            }
        }
    }
    
    Ok(())
}

/// è¿è¡Œéƒ¨ç½²éªŒè¯æµ‹è¯•
async fn run_deployment_tests(config: Config, output_dir: &str, environment: &str) -> Result<()> {
    info!("å¼€å§‹æ‰§è¡Œéƒ¨ç½²éªŒè¯æµ‹è¯• (ç¯å¢ƒ: {})", environment);
    
    let test_config = TestRunnerConfig {
        run_e2e_tests: false,
        run_deployment_validation: true,
        run_security_tests: false,
        test_timeout_seconds: 300,
        report_output_path: output_dir.to_string(),
        generate_detailed_report: true,
        max_concurrent_tests: 4,
    };
    
    let runner = IntegrationTestRunner::new(test_config, config).await?;
    let results = runner.run_comprehensive_tests().await?;
    
    if let Some(deployment_results) = results.deployment_validation {
        println!("\nğŸš€ éƒ¨ç½²éªŒè¯ç»“æœ:");
        println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        println!("   â€¢ æ€»éªŒè¯é¡¹: {}", deployment_results.validations.len());
        println!("   â€¢ é€šè¿‡éªŒè¯: {} âœ…", deployment_results.success_count());
        println!("   â€¢ å¤±è´¥éªŒè¯: {} âŒ", deployment_results.failure_count());
        
        // æ˜¾ç¤ºå¤±è´¥çš„éªŒè¯
        for (name, result) in &deployment_results.validations {
            if !result.success {
                println!("   âŒ {}: {}", name, 
                    result.error_message.as_ref().unwrap_or(&"æœªçŸ¥é”™è¯¯".to_string()));
            }
        }
    }
    
    Ok(())
}

/// è¿è¡Œå®‰å…¨æ€§æµ‹è¯•
async fn run_security_tests(config: Config, output_dir: &str, penetration_test: bool) -> Result<()> {
    info!("å¼€å§‹æ‰§è¡Œå®‰å…¨æ€§æµ‹è¯• (æ¸—é€æµ‹è¯•: {})", penetration_test);
    
    let test_config = TestRunnerConfig {
        run_e2e_tests: false,
        run_deployment_validation: false,
        run_security_tests: true,
        test_timeout_seconds: 600,
        report_output_path: output_dir.to_string(),
        generate_detailed_report: true,
        max_concurrent_tests: 2, // å®‰å…¨æµ‹è¯•ä½¿ç”¨è¾ƒå°‘å¹¶å‘
    };
    
    let runner = IntegrationTestRunner::new(test_config, config).await?;
    let results = runner.run_comprehensive_tests().await?;
    
    if let Some(security_results) = results.security_results {
        println!("\nğŸ”’ å®‰å…¨æ€§æµ‹è¯•ç»“æœ:");
        println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        println!("   â€¢ æ€»æµ‹è¯•æ•°: {}", security_results.tests.len());
        println!("   â€¢ é€šè¿‡æµ‹è¯•: {} âœ…", security_results.success_count());
        println!("   â€¢ å¤±è´¥æµ‹è¯•: {} âŒ", security_results.failure_count());
        
        // æ˜¾ç¤ºå¤±è´¥çš„å®‰å…¨æµ‹è¯•
        for (name, result) in &security_results.tests {
            if !result.success {
                println!("   ğŸš¨ {}: {}", name, 
                    result.error_message.as_ref().unwrap_or(&"æœªçŸ¥å®‰å…¨é—®é¢˜".to_string()));
            }
        }
    }
    
    Ok(())
}

/// è¿è¡Œå¥åº·æ£€æŸ¥
async fn run_health_check(config: Config) -> Result<()> {
    info!("å¼€å§‹æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥");
    
    let test_config = TestRunnerConfig::default();
    let runner = IntegrationTestRunner::new(test_config, config).await?;
    let results = runner.run_health_check().await?;
    
    println!("\nğŸ¥ ç³»ç»Ÿå¥åº·æ£€æŸ¥ç»“æœ:");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    
    for (name, result) in &results.checks {
        let status = if result.healthy { "âœ… å¥åº·" } else { "âŒ å¼‚å¸¸" };
        let response_time = format!("{:?}", result.response_time);
        println!("   â€¢ {}: {} (å“åº”æ—¶é—´: {})", name, status, response_time);
        
        if !result.healthy {
            println!("     é”™è¯¯: {}", result.message);
        }
    }
    
    println!("\n{}", results.summary());
    
    Ok(())
}

/// è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
async fn run_benchmark_tests(
    config: Config,
    output_dir: &str,
    duration: u64,
    concurrent_users: usize,
) -> Result<()> {
    info!("å¼€å§‹æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯• (æŒç»­æ—¶é—´: {}s, å¹¶å‘ç”¨æˆ·: {})", duration, concurrent_users);
    
    let test_config = TestRunnerConfig {
        test_timeout_seconds: duration + 60, // é¢å¤–ç¼“å†²æ—¶é—´
        report_output_path: output_dir.to_string(),
        generate_detailed_report: true,
        max_concurrent_tests: concurrent_users,
        ..Default::default()
    };
    
    let runner = IntegrationTestRunner::new(test_config, config).await?;
    let results = runner.run_performance_benchmark().await?;
    
    println!("\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    
    for benchmark in &results.benchmarks {
        println!("   â€¢ {}:", benchmark.name);
        println!("     - å¹³å‡è€—æ—¶: {:?}", benchmark.average_duration);
        println!("     - æœ€å°è€—æ—¶: {:?}", benchmark.min_duration);
        println!("     - æœ€å¤§è€—æ—¶: {:?}", benchmark.max_duration);
        println!("     - ååé‡: {:.2} ops/s", benchmark.throughput);
    }
    
    Ok(())
}

/// ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
async fn generate_report(format: &str, input: &std::path::Path, output_dir: &str) -> Result<()> {
    info!("ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š (æ ¼å¼: {}, è¾“å…¥: {:?})", format, input);
    
    match format.to_lowercase().as_str() {
        "html" => {
            println!("ğŸ“„ ç”Ÿæˆ HTML æ ¼å¼æŠ¥å‘Š...");
            // å®ç° HTML æŠ¥å‘Šç”Ÿæˆé€»è¾‘
        }
        "json" => {
            println!("ğŸ“„ ç”Ÿæˆ JSON æ ¼å¼æŠ¥å‘Š...");
            // å®ç° JSON æŠ¥å‘Šç”Ÿæˆé€»è¾‘
        }
        "pdf" => {
            println!("ğŸ“„ ç”Ÿæˆ PDF æ ¼å¼æŠ¥å‘Š...");
            // å®ç° PDF æŠ¥å‘Šç”Ÿæˆé€»è¾‘
        }
        _ => {
            error!("ä¸æ”¯æŒçš„æŠ¥å‘Šæ ¼å¼: {}", format);
            return Err("ä¸æ”¯æŒçš„æŠ¥å‘Šæ ¼å¼".into());
        }
    }
    
    println!("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºç›®å½•: {}", output_dir);
    Ok(())
}